import os
import json
import pandas as pd
from datetime import datetime
from pathlib import Path
import sys

# Prompt user for a date in YYYYmmDD format
if len(sys.argv) > 1 :
    date_str = sys.argv[1].strip()
else:
    date_str = input("Enter the date for the data folder (YYYYmmDD): ").strip()

# File Path Variables
Import_Folder_path = "D:/Overflow - One Drive Filling up - Temporary/Projects/Cyber Security/Crowdstrike/Data Prep"
Export_Folder_path = Path("C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Cyber Security/Crowdstrike/Source Data")
Export_Folder_path_SQL = Path("D:/Overflow - One Drive Filling up - Temporary/Projects/Cyber Security/Crowdstrike/Data Prep/" + date_str)
Import_data_folder = Path(Import_Folder_path + "/" + date_str)
File_Ouptput_Prefix = "Daily Vulnerability Export - ALL - "
File_Output_Suffix = " - combined - prepped.csv"
File_Output_Name = f"{File_Ouptput_Prefix}{date_str}{File_Output_Suffix}"
#File Path Variables - Streamlined dataset - Prepped for SQL Import
File_Ouptput_Prefix_SQL = "Daily_Vulnerability_Export_ALL_SQL_"
File_Output_Suffix_SQL = ".csv"
File_Output_Name_SQL = f"{File_Ouptput_Prefix_SQL}{date_str}{File_Output_Suffix_SQL}"


if not Import_data_folder.exists():
    raise FileNotFoundError(f"Folder '{Import_data_folder}' not found.")

# Prepare to collect all rows
open_rows = []
closed_rows = []

# Define which files to look for based on naming convention
file_prefixes = [
    "Daily Closed Vulnerability Export - ALL - ",
    "Daily Other Vulnerability Export - ALL - ",
    "Daily Windows Vulnerability Export - ALL - "
]

# Define fixed output schema for SQL file
sql_fields = [
    "SnapshotDate",
    "Customer_ID",
    "Host_ID",
    "Hostname",
    "Product",
    "Vulnerability_ID",
    "Description",
    "Status",
    "Severity",
    "Exploitability",
    "Opened_Date",
    "Closed_Date",
    "Recommended_Remediation",
    "Remediation_Details"
]

# Helper to join array fields
join_array = lambda x: "; ".join(x) if isinstance(x, list) else x

# Helper to sanitize fields
def clean_text(value):
    if isinstance(value, str):
        cleaned = value.replace('\r', ' ').replace('\n', ' ').replace('\t', ' ').strip()
        cleaned = cleaned.replace('"\r, http', ', http')
        cleaned = cleaned.replace('"\n, http', ', http')
        cleaned = cleaned.replace('"\r\n, http', ', http')
        cleaned = cleaned.replace('" , http', ', http')
        cleaned = cleaned.replace('", http', ', http')
        return cleaned
    return value

# Loop through expected files
for prefix in file_prefixes:
    json_path = Import_data_folder / f"{prefix}{date_str}.json"
    if not json_path.exists():
        print(f"Warning: File not found: {json_path.name}")
        continue

    with open(json_path, 'r', encoding='utf-8') as f:
        try:
            records = json.load(f)
        except json.JSONDecodeError as e:
            print(f"Error parsing {json_path.name}: {e}")
            continue
    
    for entry in records:
        status = entry.get('status', '').lower()
        for product in entry.get('products', []):
            base = {
                'Hostname': entry.get('hostname'),
                'LocalIP': entry.get('local_ip'),
                'HostType': entry.get('host_type'),
                'OSVersion': entry.get('os_version'),
                'MachineDomain': entry.get('machine_domain'),
                'OU': entry.get('ou'),
                'SiteName': entry.get('site_name'),
                'Product': product.get('product_name'),
                'CVE ID': entry.get('cve_id'),
                'CVE Description': clean_text(entry.get('cve_description')),
                'Status': entry.get('status'),
                'Severity': entry.get('severity'),
                'Created Date': entry.get('create_date'),
                'Closed Date': entry.get('closed_date'),
                'Closed Dwell Time': entry.get('closed_dwell_time'),
                'Base Score': entry.get('base_score'),
                'CVSS Version': entry.get('cvss_version'),
                'Vector': entry.get('vector'),
                'Vendor Advisory': entry.get('vendor_advisory'),
                'References': clean_text(entry.get('references')),
                'Group Names': join_array([g.get('name') for g in entry.get('host_groups', [])]),
                'Tags': join_array(entry.get('host_tags', [])),
                'Host ID': entry.get('host_id'),
                'Exploit status value': entry.get('exploit_status', {}).get('value'),
                'Exploit status label': entry.get('exploit_status', {}).get('label'),
                'Platform': entry.get('platform'),
                'Vulnerable Product Versions': product.get('product_name_version'),
                'Closed Product Versions': '',
                'RemediationLevel': entry.get('remediation_level'),
                'ExPRT Rating': entry.get('exprt_rating'),
                'Is Suppressed': entry.get('suppression_info', {}).get('is_suppressed'),
                'AdditionalRemediationAdvisoryUrl': '',
                'AdditionalRemediationSteps': '',
                'Is CISA KEV': entry.get('cisa_info', {}).get('is_cisa_kev'),
                'CISA KEV Due Date': entry.get('cisa_info', {}).get('due_date'),
                'CVE Published Date': entry.get('cve_published_date'),
                'Spotlight Published Date': entry.get('spotlight_published_date'),
                'Host Last Seen Within': '',
                'Cloud Service Instance ID': '',
                'OS Build': entry.get('os_build'),
                'Asset Criticality': entry.get('asset_criticality'),
                'Asset Roles': '',
                'Internet exposure': entry.get('internet_exposure'),
                'Vulnerability ID': entry.get('vulnerability_id'),
                'Vulnerability Metadata ID': entry.get('vulnerability_metadata_id'),
                'Managed By': entry.get('managed_by'),
                'Vulnerability Data Providers': join_array([p.get('provider') for p in entry.get('data_providers', [])]),
                'Third-party Asset IDs': entry.get('third_party_asset_ids'),
                'Third-party Scanner ID': join_array([p.get('scanner_id') for p in entry.get('data_providers', [])]),
                'Ports': entry.get('ports'),
                'Third-party Rating': join_array([p.get('rating') for p in entry.get('data_providers', [])]),
                'Last Scan Time': join_array([p.get('scan_time') for p in entry.get('data_providers', [])]),
                'Types': join_array(entry.get('types', [])),
                'CID': entry.get('cid'),
                'Customer': entry.get('customer_name'),
                'CWEs': join_array(entry.get('cwes', [])),
                'Vulnerability Confidence': entry.get('vulnerability_confidence'),
                'Asset Confidence Label': '',
                'Asset Subsidiaries': '',
                'Services Ports': '',
                'Services Transports': '',
                'Services Protocols': '',
                'Recommended Remediation Patch Publication Date': '',
                'Minimum Remediation': '',
                'Minimum Remediation Details': '',
                'Minimum Remediation Links': '',
                'Minimum Remediation Advisory URL': '',
                'Minimum Remediation Steps': '',
                'Patch Publication Date': product.get('patch_publication_date'),
                'Scan ID': '',
                'SnapshotDate': date_str
            }

            
            # Pull all recommended remediations with DropFlag

            seen_recommendations = set()
            if status in ['open', 'reopen']:
                for rec in product.get('recommended_remediations', []):
                    remediation = rec.get('remediation')
                    if remediation:
                        norm = remediation.strip().lower()
                        rec_row = base.copy()
                        rec_row['Recommended Remediations'] = remediation
                        rec_row['Remediation Details'] = clean_text(rec.get('detail'))
                        rec_row['Remediation Links'] = rec.get('link')
                        rec_row['Recommended Remediation Patch Publication Date'] = rec.get('patch_publication_date')
                        rec_row['DropFlag'] = 'x' if norm in seen_recommendations else ''
                        seen_recommendations.add(norm)
                        open_rows.append(rec_row)
                for rec in product.get('minimum_remediation', []):
                    min_row = base.copy()
                    min_row['Minimum Remediation'] = rec.get('remediation')
                    min_row['Minimum Remediation Details'] = clean_text(rec.get('detail'))
                    min_row['Minimum Remediation Links'] = rec.get('link')
                    min_row['Minimum Remediation Advisory URL'] = rec.get('vendor_advisory_url')
                    min_row['Minimum Remediation Steps'] = clean_text(rec.get('extra_steps'))
                    min_row['Patch Publication Date'] = rec.get('patch_publication_date')
                    open_rows.append(min_row)
            elif status == 'closed':
                if not product.get('recommended_remediations'):
                    rec_row = base.copy()
                    rec_row['DropFlag'] = ''
                    closed_rows.append(rec_row)
                else:
                    for rec in product.get('recommended_remediations', []):
                        rec_row = base.copy()
                        rec_row['Recommended Remediations'] = rec.get('remediation')
                        rec_row['Remediation Details'] = clean_text(rec.get('detail'))
                        rec_row['Remediation Links'] = rec.get('link')
                        rec_row['Recommended Remediation Patch Publication Date'] = rec.get('patch_publication_date')
                        rec_row['DropFlag'] = ''
                        closed_rows.append(rec_row)
                for rec in product.get('minimum_remediation', []):
                    min_row = base.copy()
                    min_row['Minimum Remediation'] = rec.get('remediation')
                    min_row['Minimum Remediation Details'] = clean_text(rec.get('detail'))
                    min_row['Minimum Remediation Links'] = rec.get('link')
                    min_row['Minimum Remediation Advisory URL'] = rec.get('vendor_advisory_url')
                    min_row['Minimum Remediation Steps'] = clean_text(rec.get('extra_steps'))
                    min_row['Patch Publication Date'] = rec.get('patch_publication_date')
                    min_row['DropFlag'] = ''
                    closed_rows.append(min_row)

# Combine both sets and export
all_rows = open_rows + closed_rows
if all_rows:
    df = pd.DataFrame(all_rows)
    df = df[df['DropFlag'] != 'x']
    df = df[~((df['Status'] != 'closed') & (df['Recommended Remediations'].isnull() | (df['Recommended Remediations'].str.strip() == '')))] 
    df.drop(columns=['DropFlag'], inplace=True)
    #Define column renam mapping Old name : New Name
    rename_map = {
        "CID": "Customer_ID",
        "CVE ID": "CVE_ID",
        "CVE Description": "CVE_Description",
        "Host ID": "Host_ID",
        "Exploit status label": "Exploitability",
        "Created Date": "Opened_Date",
        "Closed Date": "Closed_Date",
        "Recommended Remediations": "Recommended_Remediation",
        "Remediation Details": "Remediation_Details"
    }
    # Apply the renaming
    sql_ready_df = df.rename(columns=rename_map)
    #Normalize, reorder, and fill missing fields
    sql_ready_df = sql_ready_df.reindex(columns=sql_fields).fillna("")

    #Log Missing and Extra Columns
    missing_cols = set(sql_fields) - set(sql_ready_df.columns)
    extra_cols = set(sql_ready_df.columns) - set(sql_fields)

    if missing_cols:
        print(f"Warning: Missing columns in SQL output: {missing_cols}")
    if extra_cols:
        print(f"Note: Extra columns present but unused: {extra_cols}")



    #Export for SQL Import
    sql_ready_df.to_csv(Export_Folder_path_SQL / File_Output_Name_SQL, sep='\t', index=False)
    print(f"\nSQL ready data saved to: {Export_Folder_path_SQL / File_Output_Name_SQL}")
    #Export for PowerBI Import
    df.drop(columns=['SnapshotDate'], inplace=True)
    df.to_csv(Export_Folder_path / File_Output_Name, sep='\t', index=False)
    print(f"\nFlattened data saved to: {Export_Folder_path / File_Output_Name}")
    print(f"Total input rows: {len(all_rows)}")
    print(f"Exported to Power BI: {len(df)}")
    print(f"Exported to SQL: {len(sql_ready_df)}")
    if len(sql_ready_df.columns) != len(sql_fields):
        print("âš  Column count mismatch! SQL export may not align.")
else:
    print("\nDaily Vulnerability Export was NOT generated. Please check the input files.")

