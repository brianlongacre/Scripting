#!/usr/bin/env python3
"""
vuln_animator_from_combined_v3.py

Safer + faster batch animator for your "combined - prepped" snapshot CSVs.

Key features:
- Parse snapshot date from filename (yyyymmdd)
- Filter by date range (--since/--until) and/or cap number of files (--max-files)
- Dry run mode (--dry-run) to preview how many files will be processed
- Chunked reading (--chunksize) and column selection (--usecols default) to reduce memory
- Status filter (OPEN+REOPEN) and optional exploit label filter
- Self-contained Plotly HTML outputs (no internet required)

Usage example (PowerShell):
  python vuln_animator_from_combined_v3.py `
    --dir "C:/path/to/Source Data" `
    --pattern "Daily Vulnerability Export - ALL - *combined - prepped.csv" `
    --outdir "C:/path/to/Viz/output" `
    --status-mode open `
    --sep "`t" `
    --encoding "utf-8" `
    --since 20250101 `
    --until 20250331 `
    --chunksize 200000 `
    --dry-run

Then remove --dry-run to execute.
"""

from pathlib import Path
import argparse
import re
from collections import Counter
import pandas as pd
import plotly.express as px

def infer_yyyymmdd_str(name: str) -> str | None:
    m = re.search(r'(20\d{6})', name)
    if not m:
        return None
    raw = m.group(1)
    return raw  # keep as yyyymmdd

def yyyymmdd_to_iso(raw: str) -> str:
    return f"{raw[:4]}-{raw[4:6]}-{raw[6:]}"

def in_range(raw: str, since: str | None, until: str | None) -> bool:
    if since and raw < since:
        return False
    if until and raw > until:
        return False
    return True

def list_files(dirpath: Path, pattern: str, recursive: bool) -> list[Path]:
    if recursive:
        return sorted(dirpath.rglob(pattern))
    return sorted(dirpath.glob(pattern))

def load_and_aggregate_one(
    path: Path,
    sep: str | None,
    encoding: str | None,
    chunksize: int,
    status_mode: str,
    exploit_label: str | None,
    allcols: bool,
    skiprows: int,
    verbose: bool,
):
    """
    Stream-aggregate one file into:
      - sev_counter: {severity: count}
      - prod_counter: {product: count}
    """
    # Which columns to read (can be expanded later)
    default_usecols = ["Severity", "Product", "Status", "Exploit status label"]
    usecols = None if allcols else default_usecols

    # Choose engine: 'python' required if sep is None (auto-detect), else 'c' is fastest
    engine = "python" if sep is None else "c"
    read_kwargs = dict(
        sep=sep, engine=engine, encoding=encoding, low_memory=False, usecols=usecols, chunksize=chunksize, skiprows=skiprows
    )
    # If no chunksize, read whole file at once
    if chunksize <= 0:
        read_kwargs.pop("chunksize", None)

    sev_counter = Counter()
    prod_counter = Counter()
    total_rows = 0
    kept_rows = 0

    # Reader yields DataFrame chunks or a single DataFrame
    reader = pd.read_csv(path, **read_kwargs)

    if hasattr(reader, "__iter__") and not isinstance(reader, pd.DataFrame):
        chunks = reader
    else:
        chunks = [reader]

    for chunk in chunks:
        total_rows += len(chunk)
        # Normalize names we rely on
        # If columns are missing (unlikely in your combined-prepped), create placeholders
        for col in ["Severity", "Product", "Status"]:
            if col not in chunk.columns:
                if col == "Severity":
                    chunk[col] = "UNKNOWN"
                elif col == "Product":
                    chunk[col] = "Unknown"
                elif col == "Status":
                    chunk[col] = "OPEN"

        # Canonicalize
        chunk["Severity"] = chunk["Severity"].astype(str).str.upper()
        chunk["Status"] = chunk["Status"].astype(str).str.upper()
        chunk["Product"] = chunk["Product"].astype(str)

        # Status filtering
        if status_mode == "open":
            chunk = chunk[chunk["Status"].isin(["OPEN", "REOPEN"])]
        elif status_mode == "closed":
            chunk = chunk[chunk["Status"] == "CLOSED"]
        # else "all" keeps everything

        # Exploit filter
        if exploit_label and "Exploit status label" in chunk.columns:
            chunk = chunk[chunk["Exploit status label"].astype(str) == str(exploit_label)]

        kept_rows += len(chunk)

        # Aggregate
        sev_counter.update(chunk["Severity"].value_counts().to_dict())
        prod_counter.update(chunk["Product"].value_counts().to_dict())

    if verbose:
        print(f"[OK] {path.name}: total_rows={total_rows:,}, kept_rows={kept_rows:,}")

    return sev_counter, prod_counter, total_rows, kept_rows

def make_severity_animation(df_sev: pd.DataFrame, out_html: Path):
    df_sev = df_sev.sort_values("snapshot_date")
    fig = px.line(
        df_sev,
        x="snapshot_date",
        y="vuln_count",
        color="severity",
        markers=True,
        title="Vulnerabilities Over Time by Severity (Animated)",
        labels={"snapshot_date": "Snapshot Date", "vuln_count": "Vulnerability Count"},
        animation_frame="snapshot_date",
    )
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 350
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 200
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def make_bar_race(df_prod: pd.DataFrame, out_html: Path, top_k: int = 10):
    df_top = (
        df_prod.sort_values(["snapshot_date", "vuln_count"], ascending=[True, False])
               .groupby("snapshot_date", as_index=False)
               .head(top_k)
    )
    fig = px.bar(
        df_top.sort_values(["snapshot_date", "vuln_count"]),
        x="vuln_count",
        y="product",
        color="product",
        orientation="h",
        animation_frame="snapshot_date",
        labels={"vuln_count": "Vulnerability Count", "product": "Product"},
        title=f"Top {top_k} Products by Vulnerabilities (Bar Chart Race)",
    )
    fig.update_yaxes(categoryorder="total ascending")
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 400
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 200
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dir", required=True, help="Directory containing the combined-prepped CSV files")
    ap.add_argument("--pattern", default="Daily Vulnerability Export - ALL - *combined - prepped.csv", help="Glob file pattern (non-recursive by default)")
    ap.add_argument("--recursive", action="store_true", help="Recurse into subfolders (uses rglob)")
    ap.add_argument("--outdir", default=".", help="Output directory for HTML files")

    ap.add_argument("--status-mode", choices=["open", "closed", "all"], default="open", help="Filter by status (default: open = OPEN + REOPEN)")
    ap.add_argument("--exploit", default=None, help="Optional exact match filter for 'Exploit status label'")

    ap.add_argument("--sep", default=None, help="Delimiter override, e.g. ',' or '\\t' (PowerShell: use \"`t\" for tab)")
    ap.add_argument("--encoding", default=None, help="File encoding override, e.g. 'utf-8', 'utf-16'")
    ap.add_argument("--skiprows", type=int, default=0, help="Rows to skip at top of each file")

    ap.add_argument("--since", default=None, help="Process files with filename date >= yyyymmdd")
    ap.add_argument("--until", default=None, help="Process files with filename date <= yyyymmdd")
    ap.add_argument("--max-files", type=int, default=None, help="Process at most N files (after filters)")
    ap.add_argument("--chunksize", type=int, default=250000, help="Rows per chunk (<=0 to read whole file)")

    ap.add_argument("--allcols", action="store_true", help="Read all columns (disables usecols optimization)")
    ap.add_argument("--dry-run", action="store_true", help="List files that would be processed and exit")
    ap.add_argument("--verbose", action="store_true", help="Verbose progress logging")
    args = ap.parse_args()

    dirpath = Path(args.dir)
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    files_all = list_files(dirpath, args.pattern, args.recursive)

    # Annotate with yyyymmdd for filtering
    annotated = []
    for f in files_all:
        snap_raw = infer_yyyymmdd_str(f.name) or infer_yyyymmdd_str(str(f.parent))
        if snap_raw is None:
            continue
        if not in_range(snap_raw, args.since, args.until):
            continue
        annotated.append((f, snap_raw))

    # Apply max-files cap (process newest first)
    annotated.sort(key=lambda x: x[1])  # sort by date ascending
    if args.max_files is not None and args.max_files > 0:
        annotated = annotated[-args.max_files:]  # keep most recent N

    if args.dry_run:
        print(f"[DRY RUN] Found {len(annotated)} files matching pattern/date filters.")
        for f, d in annotated[:20]:
            print(f"  {d}  {f.name}")
        if len(annotated) > 20:
            print(f"  ... and {len(annotated)-20} more")
        return

    if not annotated:
        raise SystemExit("No files to process after applying filters. Check --pattern/--since/--until.")

    # Aggregate across all files
    sev_rows = []
    prod_rows = []

    for f, snap_raw in annotated:
        iso_date = yyyymmdd_to_iso(snap_raw)
        if args.verbose:
            print(f"[READ] {iso_date}  {f}")

        sev_counter, prod_counter, total_rows, kept_rows = load_and_aggregate_one(
            path=f,
            sep=args.sep,
            encoding=args.encoding,
            chunksize=args.chunksize,
            status_mode=args.status_mode,
            exploit_label=args.exploit,
            allcols=args.allcols,
            skiprows=args.skiprows,
            verbose=args.verbose,
        )

        for sev, cnt in sev_counter.items():
            sev_rows.append({"snapshot_date": iso_date, "severity": sev, "vuln_count": int(cnt)})
        for prod, cnt in prod_counter.items():
            prod_rows.append({"snapshot_date": iso_date, "product": prod, "vuln_count": int(cnt)})

    if not sev_rows or not prod_rows:
        raise SystemExit("No rows aggregated; check filters (status/exploit) or input columns.")

    df_sev = pd.DataFrame(sev_rows)
    df_prod = pd.DataFrame(prod_rows)

    # Build HTMLs
    make_severity_animation(df_sev, outdir / "animated_severity_trend.html")
    make_bar_race(df_prod, outdir / "animated_bar_race_products.html", top_k=10 if args.verbose is False else 10)

    print("[DONE] Wrote:")
    print(" ", outdir / "animated_severity_trend.html")
    print(" ", outdir / "animated_bar_race_products.html")

if __name__ == "__main__":
    main()
