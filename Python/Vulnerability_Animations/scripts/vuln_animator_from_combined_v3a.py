
"""
vuln_animator_from_combined_v3.py

Safer + faster batch animator for your "combined - prepped" snapshot CSVs.

Key features:
- Parse snapshot date from filename (yyyymmdd)
- Filter by date range (--since/--until) and/or cap number of files (--max-files)
- Dry run mode (--dry-run) to preview how many files will be processed
- Chunked reading (--chunksize) and column selection (--usecols default) to reduce memory
- Status filter (OPEN+REOPEN) and optional exploit label filter
- Self-contained Plotly HTML outputs (no internet required)

Usage example (PowerShell):
  python vuln_animator_from_combined_v3.py `
    --dir "C:/path/to/Source Data" `
    --pattern "Daily Vulnerability Export - ALL - *combined - prepped.csv" `
    --outdir "C:/path/to/Viz/output" `
    --status-mode open `
    --sep "`t" `
    --encoding "utf-8" `
    --since 20250101 `
    --until 20250331 `
    --chunksize 200000 `
    --dry-run

Then remove --dry-run to execute.
"""

from pathlib import Path
import argparse
import re
from collections import Counter
import pandas as pd
import plotly.express as px
from datetime import datetime

DEFAULT_SEP = "\t"         # Python tab (not PowerShell `t)
DEFAULT_ENCODING = "utf-8"  # try "utf-8-sig" or "utf-16" if needed

def normalize_sep(s: str | None) -> str | None:
    if s is None:
        return None
    # Accept common ways users might pass a tab
    if s in (r"\t", "`t"):
        return "\t"
    return s

def infer_yyyymmdd_str(name: str) -> str | None:
    m = re.search(r'(20\d{6})', name)
    if not m:
        return None
    raw = m.group(1)
    return raw  # keep as yyyymmdd

def yyyymmdd_to_iso(raw: str) -> str:
    return f"{raw[:4]}-{raw[4:6]}-{raw[6:]}"

def in_range(raw: str, since: str | None, until: str | None) -> bool:
    if since and raw < since:
        return False
    if until and raw > until:
        return False
    return True

def list_files(dirpath: Path, pattern: str, recursive: bool) -> list[Path]:
    if recursive:
        return sorted(dirpath.rglob(pattern))
    return sorted(dirpath.glob(pattern))

def load_and_aggregate_one(
    path: Path,
    sep: str | None,
    encoding: str | None,
    chunksize: int,
    status_mode: str,
    exploit_label: str | None,
    allcols: bool,
    skiprows: int,
    verbose: bool,
):
    """
    Stream-aggregate one file into:
      - sev_counter: {severity: count}
      - prod_counter: {product: count}
    """
    # Which columns to read (can be expanded later)
    default_usecols = ["Severity", "Product", "Status", "Exploit status label"]
    usecols = None if allcols else default_usecols

    # Choose engine: 'python' required if sep is None (auto-detect), else 'c' is fastest
    engine = "python" if sep is None else "c"
    read_kwargs = dict(
        sep=sep, engine=engine, encoding=encoding, low_memory=False, usecols=usecols, chunksize=chunksize, skiprows=skiprows
    )
    # If no chunksize, read whole file at once
    if chunksize <= 0:
        read_kwargs.pop("chunksize", None)

    sev_counter = Counter()
    prod_counter = Counter()
    total_rows = 0
    kept_rows = 0

    # Reader yields DataFrame chunks or a single DataFrame
    reader = pd.read_csv(path, **read_kwargs)

    if hasattr(reader, "__iter__") and not isinstance(reader, pd.DataFrame):
        chunks = reader
    else:
        chunks = [reader]

    for chunk in chunks:
        total_rows += len(chunk)
        # Normalize names we rely on
        # If columns are missing (unlikely in your combined-prepped), create placeholders
        for col in ["Severity", "Product", "Status"]:
            if col not in chunk.columns:
                if col == "Severity":
                    chunk[col] = "UNKNOWN"
                elif col == "Product":
                    chunk[col] = "Unknown"
                elif col == "Status":
                    chunk[col] = "OPEN"

        # Canonicalize
        chunk["Severity"] = chunk["Severity"].astype(str).str.upper()
        chunk["Status"] = chunk["Status"].astype(str).str.upper()
        chunk["Product"] = chunk["Product"].astype(str)

        # Status filtering
        if status_mode == "open":
            chunk = chunk[chunk["Status"].isin(["OPEN", "REOPEN"])]
        elif status_mode == "closed":
            chunk = chunk[chunk["Status"] == "CLOSED"]
        # else "all" keeps everything

        # Exploit filter
        if exploit_label and "Exploit status label" in chunk.columns:
            chunk = chunk[chunk["Exploit status label"].astype(str) == str(exploit_label)]

        kept_rows += len(chunk)

        # Aggregate
        sev_counter.update(chunk["Severity"].value_counts().to_dict())
        prod_counter.update(chunk["Product"].value_counts().to_dict())

    if verbose:
        print(f"[OK] {path.name}: total_rows={total_rows:,}, kept_rows={kept_rows:,}")

    return sev_counter, prod_counter, total_rows, kept_rows

SEV_COLORS = {
    "CRITICAL": "#d62728",  # red
    "HIGH":     "#ff7f0e",  # orange
    "MEDIUM":   "#bcbd22",  # olive
    "LOW":      "#1f77b4",  # blue
    "UNKNOWN":  "#7f7f7f",  # gray
}

def make_severity_animation(
    df_sev: pd.DataFrame,
    out_html: Path,
    theme: str,
    date_fmt: str = "%m#/%d#/%Y",
    title_suffix: str = ""
):
    
    df = df_sev.copy()
    df["snapshot_date"] = pd.to_datetime(df["snapshot_date"])
    df["severity"] = df["severity"].astype(str).str.upper()
    sev_order = ["CRITICAL", "HIGH", "MEDIUM", "LOW", "UNKNOWN"]

    # cumulative frames so the lines visibly “grow”
    dates = sorted(df["snapshot_date"].unique())
    frames = [df[df["snapshot_date"] <= d].assign(frame=d) for d in dates]
    df_anim = pd.concat(frames, ignore_index=True)

    # nice slider labels: use a string frame label in YYYY-MM-DD order
    df_anim["frame_label"] = df_anim["frame"].dt.strftime(date_fmt)
    frame_order = [pd.to_datetime(d).strftime(date_fmt) for d in dates]
    
    """
    sincedate = datetime(df["snapshot_date"].min())
    untildate = datetime(df["snapshot_date"].max())
    sincehr = sincedate.strftime("%#m/%#d/%Y")
    untilhr = untildate.strftime("%#m/%#d/%Y")
    """

    fig = px.line(
        df_anim,
        x="snapshot_date",
        y="vuln_count",
        color="severity",
        animation_frame="frame_label",  # <- human-readable labels
        category_orders={"severity": sev_order, "frame_label": frame_order},
        color_discrete_map=SEV_COLORS,
        markers=True,
        title="Vulnerabilities Over Time by Severity (Animated, Cumulative)"
              + (f" – {title_suffix}" if title_suffix else ""),
        labels={"snapshot_date": "Snapshot Date", "vuln_count": "Vulnerability Count"},
    )
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40))
    fig.update_xaxes(tickformat=date_fmt) #axis ticks: M/D/YYYY
    fig.update_yaxes(tickformat=",")
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 350
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 200
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)


def make_bar_race(
    df_prod: pd.DataFrame,
    out_html: Path,
    top_k: int = 10,
    theme: str = "plotly_dark",
    show_values: bool = True,
    date_fmt: str = "%m/%d/%Y",
    title_suffix: str = "",
):
    # Top-K per date
    df_top = (
        df_prod.sort_values(["snapshot_date", "vuln_count"], ascending=[True, False])
               .groupby("snapshot_date", as_index=False)
               .head(top_k)
               .copy()
    )

    # hygiene
    df_top["vuln_count"] = pd.to_numeric(df_top["vuln_count"], errors="coerce").fillna(0).astype(int)
    df_top["product"] = df_top["product"].astype(str).fillna("Unknown")

    # frame labels as MM/DD/YYYY for the slider
    df_top["snapshot_date"] = pd.to_datetime(df_top["snapshot_date"])
    df_top["frame_label"]   = df_top["snapshot_date"].dt.strftime(date_fmt)
    frame_order = (df_top[["snapshot_date","frame_label"]]
                     .drop_duplicates()
                     .sort_values("snapshot_date")["frame_label"].tolist())

    max_x = int(df_top["vuln_count"].max() * 1.10) if len(df_top) else 1

    fig = px.bar(
        df_top.sort_values(["snapshot_date", "vuln_count"]),
        x="vuln_count",
        y="product",
        color="product",
        orientation="h",
        animation_frame="frame_label",               # <- human-readable slider labels
        category_orders={"frame_label": frame_order},
        text="vuln_count" if show_values else None,  # <- provide actual values (fixes NaN)
        labels={"vuln_count": "Vulnerability Count", "product": "Product"},
        title=f"Top {top_k} Products by Vulnerabilities (Bar Chart Race)"
              + (f" – {title_suffix}" if title_suffix else ""),
        range_x=[0, max_x],                          # keep later frames from clipping
    )

    fig.update_yaxes(categoryorder="total ascending")
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40), yaxis=dict(automargin=True))
    fig.update_xaxes(tickformat=",")

    if show_values:
        fig.update_traces(texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    else:
        fig.update_traces(text=None, texttemplate=None)

    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 600
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 500
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

    
    fig.update_layout(yaxis=dict(automargin=True))
    fig.update_traces(text="vuln_count", texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    fig.update_yaxes(categoryorder="total ascending")
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40), yaxis=dict(automargin=True))
    fig.update_xaxes(tickformat=",")

    if show_values:
        # attach actual values before formatting to avoid NaN
        fig.update_traces(text="vuln_count")
        fig.update_traces(texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    else:
        # explicitly clear any inherited text state
        fig.update_traces(text=None, texttemplate=None) 

    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 600
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 500
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dir", required=True, help="Directory containing the combined-prepped CSV files")
    ap.add_argument("--pattern", default="Daily Vulnerability Export - ALL - *combined - prepped.csv", help="Glob file pattern (non-recursive by default)")
    ap.add_argument("--recursive", action="store_true", help="Recurse into subfolders (uses rglob)")
    ap.add_argument("--outdir", default=".", help="Output directory for HTML files")

    ap.add_argument("--status-mode", choices=["open", "closed", "all"], default="open", help="Filter by status (default: open = OPEN + REOPEN)")
    ap.add_argument("--exploit", default=None, help="Optional exact match filter for 'Exploit status label'")

    ap.add_argument("--sep", default= "\t", help="Delimiter override, e.g. ',' or '\\t' (PowerShell: use \"`t\" for tab)")
    ap.add_argument("--encoding", default="UTF-8", help="File encoding override, e.g. 'utf-8', 'utf-16'")
    ap.add_argument("--skiprows", type=int, default=0, help="Rows to skip at top of each file")

    from datetime import datetime, timedelta

    ap.add_argument("--until", default=None, help="Upper bound date (yyyymmdd). Defaults to latest file date.")
    ap.add_argument("--since", default=None, help="Lower bound date (yyyymmdd).")
    grp = ap.add_mutually_exclusive_group()
    grp.add_argument("--window-days", type=int, default=None, help="Lookback window in days (computed from --until or latest file).")
    grp.add_argument("--window-weeks", type=int, default=None, help="Lookback window in weeks (computed from --until or latest file).")

    ap.add_argument("--max-files", type=int, default=None, help="Process at most N files (after filters)")
    ap.add_argument("--chunksize", type=int, default=250000, help="Rows per chunk (<=0 to read whole file)")

    ap.add_argument("--allcols", action="store_true", help="Read all columns (disables usecols optimization)")
    ap.add_argument("--dry-run", action="store_true", help="List files that would be processed and exit")
    ap.add_argument("--verbose", action="store_true", help="Verbose progress logging")

    ap.add_argument("--theme", default="plotly_dark", choices=[
        "plotly", "plotly_dark", "simple_white", "ggplot2", "seaborn", "presentation"
    ], help="Plot theme/template")
    args = ap.parse_args()

    sep = normalize_sep(args.sep) if hasattr(args, "sep") else None
    if sep is None:
        sep = DEFAULT_SEP
    
    encoding = args.encoding or DEFAULT_ENCODING

    # choose engine: 'c' only if sep is a single non-regex char
    engine = "c" if (sep is not None and len(sep) == 1) else "python"

    dirpath = Path(args.dir)
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    files_all = list_files(dirpath, args.pattern, args.recursive)

    # Build list with (file, yyyymmdd)
    candidates = []
    for f in files_all:
        d = infer_yyyymmdd_str(f.name) or infer_yyyymmdd_str(str(f.parent))
        if d: candidates.append((f, d))

    if not candidates:
        raise SystemExit("No files with yyyymmdd in name found.")

    # Determine default UNTIL if not provided (use latest file date)
    all_dates = sorted(d for _, d in candidates)
    if args.until is None:
        args.until = all_dates[-1]

    # If a window is provided, compute SINCE from UNTIL
    if args.window_days or args.window_weeks:
        u = datetime.strptime(args.until, "%Y%m%d")
        delta = timedelta(days=args.window_days) if args.window_days else timedelta(weeks=args.window_weeks)
        s = u - delta
        args.since = s.strftime("%Y%m%d")
    
    #Human Readable Dates
    sincedate = datetime.strptime(args.since, "%Y%m%d")
    untildate = datetime.strptime(args.until, "%Y%m%d")
    
    sincehr = format(sincedate, "%#m/%#d/%Y")
    untilhr = format(untildate, "%#m/%#d/%Y")
    
    title_suffix = f"{sincehr} – {untilhr}"
    date_fmt = "%m/%d/%Y"

    # Sanity clamp
    if args.since and args.until and args.since > args.until:
        args.since, args.until = args.until, args.since  # swap if user reversed them

    # Apply date filters + max-files
    annotated = [(f, d) for (f, d) in candidates if in_range(d, args.since, args.until)]
    annotated.sort(key=lambda x: x[1])  # ascending
    if args.max_files and args.max_files > 0:
        annotated = annotated[-args.max_files:]

    """
    # Annotate with yyyymmdd for filtering
    annotated = []
    for f in files_all:
        snap_raw = infer_yyyymmdd_str(f.name) or infer_yyyymmdd_str(str(f.parent))
        if snap_raw is None:
            continue
        if not in_range(snap_raw, args.since, args.until):
            continue
        annotated.append((f, snap_raw))

    # Apply max-files cap (process newest first)
    annotated.sort(key=lambda x: x[1])  # sort by date ascending
    if args.max_files is not None and args.max_files > 0:
        annotated = annotated[-args.max_files:]  # keep most recent N
    """

    if args.dry_run:
        print(f"[DRY RUN] Found {len(annotated)} files matching pattern/date filters.")
        for f, d in annotated[:20]:
            print(f"  {d}  {f.name}")
        if len(annotated) > 20:
            print(f"  ... and {len(annotated)-20} more")
        return

    if not annotated:
        raise SystemExit("No files to process after applying filters. Check --pattern/--since/--until.")

    # Aggregate across all files
    sev_rows = []
    prod_rows = []

    for f, snap_raw in annotated:
        iso_date = yyyymmdd_to_iso(snap_raw)
        if args.verbose:
            print(f"[READ] {iso_date}  {f}")

        # reuse the normalized values we computed above main()
        sep = sep
        encoding = encoding

        sev_counter, prod_counter, total_rows, kept_rows = load_and_aggregate_one(
            path=f,
            sep=sep,
            encoding=encoding,
            chunksize=args.chunksize,
            status_mode=args.status_mode,
            exploit_label=args.exploit,
            allcols=args.allcols,
            skiprows=args.skiprows,
            verbose=args.verbose,
)

        for sev, cnt in sev_counter.items():
            sev_rows.append({"snapshot_date": iso_date, "severity": sev, "vuln_count": int(cnt)})
        for prod, cnt in prod_counter.items():
            prod_rows.append({"snapshot_date": iso_date, "product": prod, "vuln_count": int(cnt)})

    if not sev_rows or not prod_rows:
        raise SystemExit("No rows aggregated; check filters (status/exploit) or input columns.")

    df_sev = pd.DataFrame(sev_rows)
    df_prod = pd.DataFrame(prod_rows)

    # Build HTMLs
    make_severity_animation(
    df_sev,
    outdir / f"animated_severity_trend-{args.until}.html",
    theme=args.theme,
    date_fmt=date_fmt,
    title_suffix=title_suffix,
)
    make_bar_race(
    df_prod,
    outdir / f"animated_bar_race_products-{args.until}.html",
    top_k=10 if args.verbose is False else 10,
    theme=args.theme,
    show_values=True,
    date_fmt=date_fmt,
    title_suffix=title_suffix,
)

    print("[DONE] Wrote:")
    print(" ", outdir / f"animated_severity_trend-{args.until}.html")
    print(" ", outdir  / f"animated_bar_race_products-{args.until}.html")

if __name__ == "__main__":
    main()