
"""
vuln_animator_from_combined_v5.py

Safer + faster batch animator for your "combined - prepped" snapshot CSVs.

Key features:
- Parse snapshot date from filename (yyyymmdd)
- Filter by date range (--since/--until) and/or cap number of files (--max-files)
- Dry run mode (--dry-run) to preview how many files will be processed
- Chunked reading (--chunksize) and column selection (--usecols default) to reduce memory
- Status filter (OPEN+REOPEN) and optional exploit label filter
- Self-contained Plotly HTML outputs (no internet required)

Usage example (PowerShell):
  python vuln_animator_from_combined_v5.py `
    --dir "C:/path/to/Source Data" `
    --pattern "Daily Vulnerability Export - ALL - *combined - prepped.csv" `
    --outdir "C:/path/to/Viz/output" `
    --status-mode open `
    --sep "`t" `
    --encoding "utf-8" `
    --since 20250101 `
    --until 20250331 `
    --chunksize 200000 `
    --dry-run

Then remove --dry-run to execute.

----------------------------------
CLI HELP:
    --config        default=None, help="Path to config.yaml (CLI overrides config)"
    --dir 		    help="Directory containing the combined-prepped CSV files"
    --pattern	    help="Glob file pattern (non-recursive by default)"
    --recursive	    help="Recurse into subfolders (uses rglob)"
    --outdir	    help="Output directory for HTML files"
    --status-mode	choices=["open", "closed", "all"], default="open", help="Filter by status (default: open = OPEN + REOPEN)"
    --exploit	    help="Optional exact match filter for 'Exploit status label'"
    --sep		    default= "\t", help="Delimiter override, e.g. ',' or '\\t' (PowerShell: use \"`t\" for tab)"
    --encoding	    default="UTF-8", help="File encoding override, e.g. 'utf-8', 'utf-16'"
    --skiprows	    type=int, default=0, help="Rows to skip at top of each file"
    --until		    default=None, help="Upper bound date (yyyymmdd). Defaults to latest file date."
    --since		    default=None, help="Lower bound date (yyyymmdd)."
    --window-days	type=int, default=None, help="Lookback window in days (computed from --until or latest file)."
    --window-weeks	type=int, default=None, help="Lookback window in weeks (computed from --until or latest file)."
    --max-files	    type=int, default=None, help="Process at most N files (after filters)"
    --chunksize	    type=int, default=250000, help="Rows per chunk (<=0 to read whole file)"
    --allcols	    action="store_true", help="Read all columns (disables usecols optimization)"
    --dry-run	    action="store_true", help="List files that would be processed and exit"
    --topk          type=int, default=10, help="The number of Products to be displayed in the Bar Race chart"
    --verbose	    action="store_true", help="Verbose progress logging"
    --theme	        default="plotly_dark", choices=["plotly", "plotly_dark", "simple_white", "ggplot2", "seaborn", "presentation"], help="Plot theme/template"

----------------------------------

Sample default script for Powershell execution:

python "C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Scripting/Python/Vulnerability_Animations/scripts/vuln_animator_from_combined_v4.py" `
  --dir "C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Cyber Security/Crowdstrike/Source Data" `
  --pattern "Daily Vulnerability Export - ALL - *combined - prepped.csv" `
  --outdir "C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Cyber Security/Crowdstrike/Viz/output" `
  --until "20250922" `
  --status-mode open `
  --window-weeks 13 `
  --chunksize 200000 `
  --verbose

----------------------------------

v5 adds:
    Persistant color mapping for products within the Bar Race chart
    Ability to leverage config.yaml for default values, these can be overridden by cli options

"""

from pathlib import Path
import argparse
import re
from collections import Counter
import json
import pandas as pd
import plotly.express as px
from plotly.colors import qualitative as qpal
from datetime import datetime

DEFAULT_SEP = "\t"         # Python tab (not PowerShell `t)
DEFAULT_ENCODING = "utf-8"  # try "utf-8-sig" or "utf-16" if needed

def normalize_sep(s: str | None) -> str | None:
    if s is None:
        return None
    # Accept common ways users might pass a tab
    if s in (r"\t", "`t"):
        return "\t"
    return s

def infer_yyyymmdd_str(name: str) -> str | None:
    m = re.search(r'(20\d{6})', name)
    if not m:
        return None
    raw = m.group(1)
    return raw  # keep as yyyymmdd

def yyyymmdd_to_iso(raw: str) -> str:
    return f"{raw[:4]}-{raw[4:6]}-{raw[6:]}"

def in_range(raw: str, since: str | None, until: str | None) -> bool:
    if since and raw < since:
        return False
    if until and raw > until:
        return False
    return True

def list_files(dirpath: Path, pattern: str, recursive: bool) -> list[Path]:
    if recursive:
        return sorted(dirpath.rglob(pattern))
    return sorted(dirpath.glob(pattern))

def load_and_aggregate_one(
    path: Path,
    sep: str | None,
    encoding: str | None,
    chunksize: int,
    status_mode: str,
    exploit_label: str | None,
    allcols: bool,
    skiprows: int,
    verbose: bool,
):
    """
    Stream-aggregate one file into:
      - sev_counter: {severity: count}
      - prod_counter: {product: count}
    """
    # Which columns to read (can be expanded later)
    default_usecols = ["Severity", "Product", "Status", "Exploit status label"]
    usecols = None if allcols else default_usecols

    # Choose engine: 'python' required if sep is None (auto-detect), else 'c' is fastest
    engine = "python" if sep is None else "c"
    read_kwargs = dict(
        sep=sep,
        engine=engine,
        encoding=encoding,
        low_memory=False,
        usecols=usecols,
        chunksize=chunksize,
        skiprows=skiprows
    )

    # If no chunksize, read whole file at once
    if chunksize <= 0:
        read_kwargs.pop("chunksize", None)

    sev_counter = Counter()
    prod_counter = Counter()
    total_rows = 0
    kept_rows = 0

    # Reader yields DataFrame chunks or a single DataFrame
    reader = pd.read_csv(path, **read_kwargs)

    if hasattr(reader, "__iter__") and not isinstance(reader, pd.DataFrame):
        chunks = reader
    else:
        chunks = [reader]

    for chunk in chunks:
        total_rows += len(chunk)
        # Normalize names we rely on
        # If columns are missing (unlikely in your combined-prepped), create placeholders
        for col in ["Severity", "Product", "Status"]:
            if col not in chunk.columns:
                if col == "Severity":
                    chunk[col] = "UNKNOWN"
                elif col == "Product":
                    chunk[col] = "Unknown"
                elif col == "Status":
                    chunk[col] = "OPEN"

        # Canonicalize
        chunk["Severity"] = chunk["Severity"].astype(str).str.upper()
        chunk["Status"] = chunk["Status"].astype(str).str.upper()
        chunk["Product"] = chunk["Product"].astype(str)

        # Status filtering
        if status_mode == "open":
            chunk = chunk[chunk["Status"].isin(["OPEN", "REOPEN"])]
        elif status_mode == "closed":
            chunk = chunk[chunk["Status"] == "CLOSED"]
        # else "all" keeps everything

        # Exploit filter
        if exploit_label and "Exploit status label" in chunk.columns:
            chunk = chunk[chunk["Exploit status label"].astype(str) == str(exploit_label)]

        kept_rows += len(chunk)

        # Aggregate
        sev_counter.update(chunk["Severity"].value_counts().to_dict())
        prod_counter.update(chunk["Product"].value_counts().to_dict())

    if verbose:
        print(f"[OK] {path.name}: total_rows={total_rows:,}, kept_rows={kept_rows:,}")

    return sev_counter, prod_counter, total_rows, kept_rows

SEV_COLORS = {
    "CRITICAL": "#d62728",  # red
    "HIGH":     "#ff7f0e",  # orange
    "MEDIUM":   "#bcbd22",  # olive
    "LOW":      "#1f77b4",  # blue
    "UNKNOWN":  "#7f7f7f",  # gray
}

def make_severity_animation(
    df_sev: pd.DataFrame,
    out_html: Path,
    theme: str,
    date_fmt: str = "%m#/%d#/%Y",
    title_suffix: str = ""
):
    
    df = df_sev.copy()
    df["snapshot_date"] = pd.to_datetime(df["snapshot_date"])
    df["severity"] = df["severity"].astype(str).str.upper()
    sev_order = ["CRITICAL", "HIGH", "MEDIUM", "LOW", "UNKNOWN"]

    # cumulative frames so the lines visibly “grow”
    dates = sorted(df["snapshot_date"].unique())
    frames = [df[df["snapshot_date"] <= d].assign(frame=d) for d in dates]
    df_anim = pd.concat(frames, ignore_index=True)

    # nice slider labels: use a string frame label in YYYY-MM-DD order
    df_anim["frame_label"] = df_anim["frame"].dt.strftime(date_fmt)
    frame_order = [pd.to_datetime(d).strftime(date_fmt) for d in dates]

    fig = px.line(
        df_anim,
        x="snapshot_date",
        y="vuln_count",
        color="severity",
        animation_frame="frame_label",  # <- human-readable labels
        category_orders={"severity": sev_order, "frame_label": frame_order},
        color_discrete_map=SEV_COLORS,
        markers=True,
        title="Vulnerabilities Over Time by Severity (Animated, Cumulative)"
              + (f" – {title_suffix}" if title_suffix else ""),
        labels={"snapshot_date": "Snapshot Date", "vuln_count": "Vulnerability Count"},
    )
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40))
    fig.update_xaxes(tickformat=date_fmt) #axis ticks: M/D/YYYY
    fig.update_yaxes(tickformat=",")
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 350
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 200
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def load_product_colormap(path: Path) -> dict:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def save_product_colormap(path: Path, cmap: dict):
    path.write_text(json.dumps(cmap, indent=2, ensure_ascii=False), encoding="utf-8")

def build_product_colormap(products: list[str], base_map: dict | None = None) -> dict:
    """
    Deterministic mapping: sort names, assign colors round-robin across big palettes.
    """
    palette = (
        qpal.Dark24 + qpal.Set3 + qpal.Safe + qpal.D3 + qpal.T10 + qpal.Pastel
    )
    cmap = dict(base_map or {})
    assigned = set(cmap.keys())
    i = 0 if not cmap else sum(palette.index(v) if v in palette else 0 for v in cmap.values()) % len(palette)

    for p in sorted(set(products)):
        if p in assigned:
            continue
        cmap[p] = palette[i % len(palette)]
        i += 1
    return cmap


def make_open_closed_stacked(df_oc: pd.DataFrame, out_html: Path, theme: str, date_fmt: str):
    if df_oc.empty:
        return
    df = df_oc.copy()
    df["snapshot_date"] = pd.to_datetime(df["snapshot_date"])
    df["frame_label"] = df["snapshot_date"].dt.strftime(date_fmt)
    frame_order = (df[["snapshot_date","frame_label"]]
                   .drop_duplicates().sort_values("snapshot_date")["frame_label"].tolist())
    sev_order = ["CRITICAL","HIGH","MEDIUM","LOW","UNKNOWN"]

    fig = px.bar(
        df.sort_values(["snapshot_date","Severity"]),
        x="Severity", y="count", color="Status",
        animation_frame="frame_label",
        category_orders={"Severity": sev_order, "frame_label": frame_order},
        labels={"count":"Vulnerability Count"},
        title="Open vs Closed by Severity (Animated, Stacked)",
    )
    fig.update_layout(barmode="stack", template=theme, margin=dict(l=40,r=20,t=60,b=40))
    fig.update_yaxes(tickformat=",")
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 450
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 250
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)


def make_bar_race(
    df_prod: pd.DataFrame,
    out_html: Path,
    top_k: int = 10,
    theme: str = "plotly_dark",
    show_values: bool = True,
    date_fmt: str = "%m/%d/%Y",
    title_suffix: str = "",
    color_map: dict | None = None,
):
    # Top-K per date
    df_top = (
        df_prod.sort_values(["snapshot_date", "vuln_count"], ascending=[True, False])
               .groupby("snapshot_date", as_index=False)
               .head(top_k)
               .copy()
    )

    # hygiene
    df_top["vuln_count"] = pd.to_numeric(df_top["vuln_count"], errors="coerce").fillna(0).astype(int)
    df_top["product"] = df_top["product"].astype(str).fillna("Unknown")

    # frame labels as MM/DD/YYYY for the slider
    df_top["snapshot_date"] = pd.to_datetime(df_top["snapshot_date"])
    df_top["frame_label"]   = df_top["snapshot_date"].dt.strftime(date_fmt)
    frame_order = (df_top[["snapshot_date","frame_label"]]
                     .drop_duplicates()
                     .sort_values("snapshot_date")["frame_label"].tolist())

    max_x = int(df_top["vuln_count"].max() * 1.10) if len(df_top) else 1

    fig = px.bar(
        df_top.sort_values(["snapshot_date", "vuln_count"]),
        x="vuln_count",
        y="product",
        color="product",
        color_discrete_map=color_map or {},
        orientation="h",
        animation_frame="frame_label",               # <- human-readable slider labels
        category_orders={"frame_label": frame_order},
        text="vuln_count" if show_values else None,  # <- provide actual values (fixes NaN)
        labels={"vuln_count": "Vulnerability Count", "product": "Product"},
        title=f"Top {top_k} Products by Vulnerabilities (Bar Chart Race)"
              + (f" – {title_suffix}" if title_suffix else ""),
        range_x=[0, max_x],                          # keep later frames from clipping
    )

    fig.update_yaxes(categoryorder="total ascending")
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40), yaxis=dict(automargin=True))
    fig.update_xaxes(tickformat=",")

    if show_values:
        fig.update_traces(texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    else:
        fig.update_traces(text=None, texttemplate=None)

    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 600
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 500
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

    
    fig.update_layout(yaxis=dict(automargin=True))
    fig.update_traces(text="vuln_count", texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    fig.update_yaxes(categoryorder="total ascending")
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40), yaxis=dict(automargin=True))
    fig.update_xaxes(tickformat=",")

    if show_values:
        # attach actual values before formatting to avoid NaN
        fig.update_traces(text="vuln_count")
        fig.update_traces(texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    else:
        # explicitly clear any inherited text state
        fig.update_traces(text=None, texttemplate=None) 

    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 600
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 500
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default="C:/users/blongacr/OneDrive - Russel Metals/Documents/Projects/Scripting/Python/Vulnerability_Animations/scripts/config.yaml", help="Path to config.yaml (CLI overrides config)")
    ap.add_argument("--dir", required=True, help="Directory containing the combined-prepped CSV files")
    ap.add_argument("--pattern", default="Daily Vulnerability Export - ALL - *combined - prepped.csv", help="Glob file pattern (non-recursive by default)")
    ap.add_argument("--recursive", action="store_true", help="Recurse into subfolders (uses rglob)")
    ap.add_argument("--outdir", default=".", help="Output directory for HTML files")

    ap.add_argument("--status-mode", choices=["open", "closed", "all"], default="open", help="Filter by status (default: open = OPEN + REOPEN)")
    ap.add_argument("--exploit", default=None, help="Optional exact match filter for 'Exploit status label'")

    ap.add_argument("--sep", default= "\t", help="Delimiter override, e.g. ',' or '\\t' (PowerShell: use \"`t\" for tab)")
    ap.add_argument("--encoding", default="UTF-8", help="File encoding override, e.g. 'utf-8', 'utf-16'")
    ap.add_argument("--skiprows", type=int, default=0, help="Rows to skip at top of each file")

    from datetime import datetime, timedelta

    ap.add_argument("--until", default=None, help="Upper bound date (yyyymmdd). Defaults to latest file date.")
    ap.add_argument("--since", default=None, help="Lower bound date (yyyymmdd).")
    grp = ap.add_mutually_exclusive_group()
    grp.add_argument("--window-days", type=int, default=None, help="Lookback window in days (computed from --until or latest file).")
    grp.add_argument("--window-weeks", type=int, default=None, help="Lookback window in weeks (computed from --until or latest file).")

    ap.add_argument("--max-files", type=int, default=None, help="Process at most N files (after filters)")
    ap.add_argument("--chunksize", type=int, default=250000, help="Rows per chunk (<=0 to read whole file)")

    ap.add_argument("--allcols", action="store_true", help="Read all columns (disables usecols optimization)")
    ap.add_argument("--dry-run", action="store_true", help="List files that would be processed and exit")
    ap.add_argument("--verbose", action="store_true", help="Verbose progress logging")

    ap.add_argument("--theme", default="plotly_dark", choices=[
        "plotly", "plotly_dark", "simple_white", "ggplot2", "seaborn", "presentation"
    ], help="Plot theme/template")
    args = ap.parse_args()

    cfg = {}
    if args.config:
        try:
            import yaml  # pip install pyyaml
            with open(args.config, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f) or {}
        except Exception as e:
            print(f"[WARN] Failed to load config: {e}")

    def choose(cli, *cfg_keys, default=None):
        cur = cli if cli not in (None, "", False) else None
        if cur is not None: return cur
        node = cfg
        for k in cfg_keys:
            node = node.get(k, {}) if isinstance(node, dict) else {}
        return node if node not in (None, "", False, {}) else default

    # Use it
    dirpath = Path(choose(args.dir, "paths", "dir"))
    outdir  = Path(choose(args.outdir, "paths", "outdir", default="."))
    pattern = choose(args.pattern, "pattern", default=args.pattern)
    theme   = choose(args.theme, "theme", default=args.theme)
    date_fmt = choose(None, "date_format", default="%m/%d/%Y")
    top_k  = int(choose(None, "topk", default=10))
    show_values = bool(choose(None, "show_values", default=True))
    status_mode = choose(args.status_mode, "status_mode", default=args.status_mode)
    sep = normalize_sep(choose(args.sep, "sep", default=DEFAULT_SEP))
    encoding = choose(args.encoding, "encoding", default=DEFAULT_ENCODING)
    window_weeks = choose(args.window_weeks, "window-weeks",default=13)


    sep = normalize_sep(args.sep) if hasattr(args, "sep") else None
    if sep is None:
        sep = DEFAULT_SEP
    
    encoding = args.encoding or DEFAULT_ENCODING

    # choose engine: 'c' only if sep is a single non-regex char
    engine = "c" if (sep is not None and len(sep) == 1) else "python"

    dirpath = Path(args.dir)
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    files_all = list_files(dirpath, args.pattern, args.recursive)

    # Build list with (file, yyyymmdd)
    candidates = []
    for f in files_all:
        d = infer_yyyymmdd_str(f.name) or infer_yyyymmdd_str(str(f.parent))
        if d: candidates.append((f, d))

    if not candidates:
        raise SystemExit("No files with yyyymmdd in name found.")

    # Determine default UNTIL if not provided (use latest file date)
    all_dates = sorted(d for _, d in candidates)
    if args.until is None:
        args.until = all_dates[-1]

    # If a window is provided, compute SINCE from UNTIL
    if args.window_days or args.window_weeks:
        u = datetime.strptime(args.until, "%Y%m%d")
        delta = timedelta(days=args.window_days) if args.window_days else timedelta(weeks=args.window_weeks)
        s = u - delta
        args.since = s.strftime("%Y%m%d")
    
    #Human Readable Dates
    sincedate = datetime.strptime(args.since, "%Y%m%d")
    untildate = datetime.strptime(args.until, "%Y%m%d")
    
    sincehr = format(sincedate, "%#m/%#d/%Y")
    untilhr = format(untildate, "%#m/%#d/%Y")
    
    title_suffix = f"{sincehr} – {untilhr}"
    date_fmt = "%m/%d/%Y"

    # Sanity clamp
    if args.since and args.until and args.since > args.until:
        args.since, args.until = args.until, args.since  # swap if user reversed them

    # Apply date filters + max-files
    annotated = [(f, d) for (f, d) in candidates if in_range(d, args.since, args.until)]
    annotated.sort(key=lambda x: x[1])  # ascending
    if args.max_files and args.max_files > 0:
        annotated = annotated[-args.max_files:]

    # Build open/closed by severity per date (in one pass through the files)
    oc_rows = []
    for f, snap_raw in annotated:
        iso_date = yyyymmdd_to_iso(snap_raw)
        reader = pd.read_csv(
            f, sep=sep, engine=("c" if len(sep)==1 else "python"),
            encoding=encoding, low_memory=False,
            usecols=["Severity","Status"], chunksize=250000
        )
        if not hasattr(reader, "__iter__"):
            reader = [reader]
        for chunk in reader:
            chunk["Severity"] = chunk["Severity"].astype(str).str.upper().fillna("UNKNOWN")
            chunk["Status"] = chunk["Status"].astype(str).str.upper().fillna("OPEN")
            chunk["Status"] = chunk["Status"].replace({"REOPEN":"OPEN"})
            g = (chunk.groupby(["Severity","Status"]).size()
                    .reset_index(name="count"))
            g["snapshot_date"] = iso_date
            oc_rows.append(g)

    df_oc = pd.concat(oc_rows, ignore_index=True) if oc_rows else pd.DataFrame(columns=["snapshot_date","Severity","Status","count"])

    if args.dry_run:
        print(f"[DRY RUN] Found {len(annotated)} files matching pattern/date filters.")
        for f, d in annotated[:20]:
            print(f"  {d}  {f.name}")
        if len(annotated) > 20:
            print(f"  ... and {len(annotated)-20} more")
        return

    if not annotated:
        raise SystemExit("No files to process after applying filters. Check --pattern/--since/--until.")

    # Aggregate across all files
    sev_rows = []
    prod_rows = []

    for f, snap_raw in annotated:
        iso_date = yyyymmdd_to_iso(snap_raw)
        if args.verbose:
            print(f"[READ] {iso_date}  {f}")

        # reuse the normalized values we computed above main()
        sep = sep
        encoding = encoding

        sev_counter, prod_counter, total_rows, kept_rows = load_and_aggregate_one(
            path=f,
            sep=sep,
            encoding=encoding,
            chunksize=args.chunksize,
            status_mode=args.status_mode,
            exploit_label=args.exploit,
            allcols=args.allcols,
            skiprows=args.skiprows,
            verbose=args.verbose,
)



        for sev, cnt in sev_counter.items():
            sev_rows.append({"snapshot_date": iso_date, "severity": sev, "vuln_count": int(cnt)})
        for prod, cnt in prod_counter.items():
            prod_rows.append({"snapshot_date": iso_date, "product": prod, "vuln_count": int(cnt)})

    if not sev_rows or not prod_rows:
        raise SystemExit("No rows aggregated; check filters (status/exploit) or input columns.")

    df_sev = pd.DataFrame(sev_rows)
    df_prod = pd.DataFrame(prod_rows)

    # Build/load persistent product color map for the current window
    color_json = outdir / "product_colors.json"
    base_map = load_product_colormap(color_json)
    window_products = df_prod["product"].astype(str).dropna().unique().tolist()
    prod_cmap = build_product_colormap(window_products, base_map)
    save_product_colormap(color_json, prod_cmap)

    # Build HTMLs
    make_severity_animation(
    df_sev,
    outdir / f"animated_severity_trend-{args.until}.html",
    theme=args.theme,
    date_fmt=date_fmt,
    title_suffix=title_suffix,
)
    
    make_open_closed_stacked(
    df_oc,
    outdir / f"open_closed_by_severity-{args.until}.html",
    theme=theme,
    date_fmt=date_fmt
)




    make_bar_race(
    df_prod,
    outdir / f"animated_bar_race_products-{args.until}.html",
    top_k=10 if args.verbose is False else 10,
    theme=args.theme,
    show_values=True,
    date_fmt=date_fmt,
    title_suffix=title_suffix,
    color_map = prod_cmap
)

    

    print("[DONE] Wrote:")
    print(" ", outdir / f"animated_severity_trend-{args.until}.html")
    print(" ", outdir  / f"animated_bar_race_products-{args.until}.html")

if __name__ == "__main__":
    main()