
"""
vuln_animator_from_combined_v6.py

Safer + faster batch animator for your "combined - prepped" snapshot CSVs.

Key features:
- Parse snapshot date from filename (yyyymmdd)
- Filter by date range (--since/--until) and/or cap number of files (--max-files)
- Dry run mode (--dry-run) to preview how many files will be processed
- Chunked reading (--chunksize) and column selection (--usecols default) to reduce memory
- Status filter (OPEN+REOPEN) and optional exploit label filter
- Self-contained Plotly HTML outputs (no internet required)

Usage example (PowerShell):
  python vuln_animator_from_combined_v6.py `
    --dir "C:/path/to/Source Data" `
    --pattern "Daily Vulnerability Export - ALL - *combined - prepped.csv" `
    --outdir "C:/path/to/Viz/output" `
    --status-mode open `
    --sep "`t" `
    --encoding "utf-8" `
    --since 20250101 `
    --until 20250331 `
    --chunksize 200000 `
    --dry-run

Then remove --dry-run to execute.

----------------------------------
CLI HELP:
    --config        default=None, help="Path to config.yaml (CLI overrides config)"
    --dir 		    help="Directory containing the combined-prepped CSV files"
    --pattern	    help="Glob file pattern (non-recursive by default)"
    --recursive	    help="Recurse into subfolders (uses rglob)"
    --outdir	    help="Output directory for HTML files"
    --status-mode	choices=["open", "closed", "all"], default="open", help="Filter by status (default: open = OPEN + REOPEN)"
    --exploit	    help="Optional exact match filter for 'Exploit status label'"
    --sep		    default= "\t", help="Delimiter override, e.g. ',' or '\\t' (PowerShell: use \"`t\" for tab)"
    --encoding	    default="UTF-8", help="File encoding override, e.g. 'utf-8', 'utf-16'"
    --skiprows	    type=int, default=0, help="Rows to skip at top of each file"
    --until		    default=None, help="Upper bound date (yyyymmdd). Defaults to latest file date."
    --since		    default=None, help="Lower bound date (yyyymmdd)."
    --window_days	type=int, default=None, help="Lookback window in days (computed from --until or latest file)."
    --window_weeks	type=int, default=None, help="Lookback window in weeks (computed from --until or latest file)."
    --max-files	    type=int, default=None, help="Process at most N files (after filters)"
    --chunksize	    type=int, default=250000, help="Rows per chunk (<=0 to read whole file)"
    --allcols	    action="store_true", help="Read all columns (disables usecols optimization)"
    --dry-run	    action="store_true", help="List files that would be processed and exit"
    --topk          type=int, default=10, help="The number of Products to be displayed in the Bar Race chart"
    --verbose	    action="store_true", help="Verbose progress logging"
    --theme	        default="plotly_dark", choices=["plotly", "plotly_dark", "simple_white", "ggplot2", "seaborn", "presentation"], help="Plot theme/template"

----------------------------------

Sample default script for Powershell execution:

python "C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Scripting/Python/Vulnerability_Animations/scripts/vuln_animator_from_combined_v4.py" `
  --dir "C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Cyber Security/Crowdstrike/Source Data" `
  --pattern "Daily Vulnerability Export - ALL - *combined - prepped.csv" `
  --outdir "C:/Users/blongacr/OneDrive - Russel Metals/Documents/Projects/Cyber Security/Crowdstrike/Viz/output" `
  --until "20250922" `
  --status_mode open `
  --window_weeks 13 `
  --chunksize 200000 `
  --verbose

----------------------------------

v5 adds:
    Persistant color mapping for products within the Bar Race chart
    Ability to leverage config.yaml for default values, these can be overridden by cli options


v6 vuln_animator_from_combined_v6.py

Safer + faster batch animator for your "combined - prepped" snapshot CSVs.

- Snapshot date parsed from filename (yyyymmdd)
- Windowing (--since/--until, --window_days, --window_weeks)
- Chunked reading and minimal columns for speed
- Status filtering (open/closed/all), optional exploit label
- Stable product colors across runs (JSON persisted, limited to Top-K)
- Config file (config.yaml) with CLI override
- Three HTMLs: severity trend (cumulative), product bar-race, open/closed stacked

Requires: pandas, plotly, pyyaml (optional but recommended)
"""

from pathlib import Path
import argparse
import re
import json
from collections import Counter
import os, platform
from datetime import datetime, timedelta

import pandas as pd
import plotly.express as px
from plotly.colors import qualitative as qpal

# -------------------------
# Defaults / helpers
# -------------------------
DEFAULT_SEP = "\t"
DEFAULT_ENCODING = "utf-8"

def normalize_sep(s: str | None) -> str | None:
    if s is None:
        return None
    if s in (r"\t", "`t"):
        return "\t"
    return s

def infer_yyyymmdd_str(name: str) -> str | None:
    m = re.search(r"(20\d{6})", name)
    return m.group(1) if m else None

def yyyymmdd_to_iso(raw: str) -> str:
    return f"{raw[:4]}-{raw[4:6]}-{raw[6:]}"

def in_range(raw: str, since: str | None, until: str | None) -> bool:
    if since and raw < since: return False
    if until and raw > until: return False
    return True

def list_files(dirpath: Path, pattern: str, recursive: bool) -> list[Path]:
    return sorted(dirpath.rglob(pattern) if recursive else dirpath.glob(pattern))

def date_fmt_python(no_leading_zero=True) -> str:
    """For Python strftime (frame labels, titles)."""
    if not no_leading_zero:
        return "%m/%d/%Y"
    return "%#m/%#d/%Y" if os.name == "nt" or platform.system() == "Windows" else "%-m/%-d/%Y"

def date_fmt_d3(no_leading_zero=True) -> str:
    """For Plotly/D3 axis ticks (browser)."""
    return "%-m/%-d/%Y" if no_leading_zero else "%m/%d/%Y"

# -------------------------
# CSV aggregation
# -------------------------
def load_and_aggregate_one(
    path: Path,
    sep: str | None,
    encoding: str | None,
    chunksize: int,
    status_mode: str,
    exploit_label: str | None,
    allcols: bool,
    skiprows: int,
    verbose: bool,
):
    default_usecols = ["Severity", "Product", "Status", "Exploit status label"]
    usecols = None if allcols else default_usecols

    # 'c' engine works only for single-char sep; fallback to python otherwise
    engine = "c" if (sep is not None and len(sep) == 1) else "python"
    read_kwargs = dict(
        sep=sep, engine=engine, encoding=encoding, low_memory=False,
        usecols=usecols, chunksize=chunksize if chunksize > 0 else None,
        skiprows=skiprows
    )

    sev_counter, prod_counter = Counter(), Counter()
    total_rows = kept_rows = 0

    reader = pd.read_csv(path, **read_kwargs)
    chunks = reader if (hasattr(reader, "__iter__") and not isinstance(reader, pd.DataFrame)) else [reader]

    for chunk in chunks:
        total_rows += len(chunk)

        for col in ["Severity", "Product", "Status"]:
            if col not in chunk.columns:
                chunk[col] = "UNKNOWN" if col == "Severity" else ("Unknown" if col == "Product" else "OPEN")

        chunk["Severity"] = chunk["Severity"].astype(str).str.upper()
        chunk["Status"]   = chunk["Status"].astype(str).str.upper()
        chunk["Product"]  = chunk["Product"].astype(str)

        if status_mode == "open":
            chunk = chunk[chunk["Status"].isin(["OPEN", "REOPEN"])]
        elif status_mode == "closed":
            chunk = chunk[chunk["Status"] == "CLOSED"]

        if exploit_label and "Exploit status label" in chunk.columns:
            chunk = chunk[chunk["Exploit status label"].astype(str) == str(exploit_label)]

        kept_rows += len(chunk)
        sev_counter.update(chunk["Severity"].value_counts().to_dict())
        prod_counter.update(chunk["Product"].value_counts().to_dict())

    if verbose:
        print(f"[OK] {path.name}: total_rows={total_rows:,}, kept_rows={kept_rows:,}")

    return sev_counter, prod_counter, total_rows, kept_rows

SEV_COLORS = {
    "CRITICAL": "#d62728",
    "HIGH":     "#ff7f0e",
    "MEDIUM":   "#bcbd22",
    "LOW":      "#1f77b4",
    "UNKNOWN":  "#7f7f7f",
}

# -------------------------
# Color map persistence
# -------------------------
def load_product_colormap(path: Path) -> dict:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def save_product_colormap(path: Path, cmap: dict):
    path.write_text(json.dumps(cmap, indent=2, ensure_ascii=False), encoding="utf-8")

def build_product_colormap(products: list[str], base_map: dict | None = None) -> dict:
    palette = qpal.Dark24 + qpal.Set3 + qpal.Safe + qpal.D3 + qpal.T10 + qpal.Pastel
    cmap = dict(base_map or {})
    assigned = set(cmap.keys())
    i = 0 if not cmap else sum((palette.index(v) if v in palette else 0) for v in cmap.values()) % len(palette)
    for p in sorted(set(products)):
        if p in assigned: continue
        cmap[p] = palette[i % len(palette)]
        i += 1
    return cmap

# -------------------------
# Charts
# -------------------------
def make_severity_animation(
    df_sev: pd.DataFrame,
    out_html: Path,
    theme: str,
    date_fmt_py: str,      # Python strftime (slider labels & title)
    date_fmt_d3_str: str,  # D3 format (axis ticks)
    title_suffix: str = "",
):
    df = df_sev.copy()
    df["snapshot_date"] = pd.to_datetime(df["snapshot_date"])
    df["severity"] = df["severity"].astype(str).str.upper()
    sev_order = ["CRITICAL", "HIGH", "MEDIUM", "LOW", "UNKNOWN"]

    dates = sorted(df["snapshot_date"].unique())
    frames = [df[df["snapshot_date"] <= d].assign(frame=d) for d in dates]
    df_anim = pd.concat(frames, ignore_index=True)

    df_anim["frame_label"] = df_anim["frame"].dt.strftime(date_fmt_py)
    frame_order = [pd.to_datetime(d).strftime(date_fmt_py) for d in dates]

    fig = px.line(
        df_anim,
        x="snapshot_date",
        y="vuln_count",
        color="severity",
        animation_frame="frame_label",
        category_orders={"severity": sev_order, "frame_label": frame_order},
        color_discrete_map=SEV_COLORS,
        markers=True,
        title="Vulnerabilities Over Time by Severity (Animated, Cumulative)"
              + (f" – {title_suffix}" if title_suffix else ""),
        labels={"snapshot_date": "Snapshot Date", "vuln_count": "Vulnerability Count"},
    )
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40))
    fig.update_xaxes(tickformat=date_fmt_d3_str)
    fig.update_yaxes(tickformat=",")
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 350
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 200
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def make_bar_race(
    df_prod: pd.DataFrame,
    out_html: Path,
    top_k: int = 10,
    theme: str = "plotly_dark",
    show_values: bool = True,
    date_fmt_py: str = "%m/%d/%Y",
    title_suffix: str = "",
    color_map: dict | None = None,
):
    df_top = (
        df_prod.sort_values(["snapshot_date", "vuln_count"], ascending=[True, False])
               .groupby("snapshot_date", as_index=False)
               .head(top_k)
               .copy()
    )
    df_top["vuln_count"] = pd.to_numeric(df_top["vuln_count"], errors="coerce").fillna(0).astype(int)
    df_top["product"] = df_top["product"].astype(str).fillna("Unknown")

    df_top["snapshot_date"] = pd.to_datetime(df_top["snapshot_date"])
    df_top["frame_label"]   = df_top["snapshot_date"].dt.strftime(date_fmt_py)
    frame_order = (df_top[["snapshot_date","frame_label"]]
                   .drop_duplicates()
                   .sort_values("snapshot_date")["frame_label"].tolist())

    max_x = int(df_top["vuln_count"].max() * 1.10) if len(df_top) else 1

    fig = px.bar(
        df_top.sort_values(["snapshot_date", "vuln_count"]),
        x="vuln_count",
        y="product",
        color="product",
        color_discrete_map=color_map or {},
        orientation="h",
        animation_frame="frame_label",
        category_orders={"frame_label": frame_order},
        text="vuln_count" if show_values else None,
        labels={"vuln_count": "Vulnerability Count", "product": "Product"},
        title=f"Top {top_k} Products by Vulnerabilities (Bar Chart Race)"
              + (f" – {title_suffix}" if title_suffix else ""),
        range_x=[0, max_x],
    )
    fig.update_yaxes(categoryorder="total ascending")
    fig.update_layout(template=theme, margin=dict(l=40, r=20, t=60, b=40), yaxis=dict(automargin=True))
    fig.update_xaxes(tickformat=",")
    if show_values:
        fig.update_traces(texttemplate="%{text:,}", textposition="outside", cliponaxis=False)
    else:
        fig.update_traces(text=None, texttemplate=None)
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 600
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 500
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

def make_open_closed_stacked(df_oc: pd.DataFrame, out_html: Path, theme: str, date_fmt_py: str, title_suffix: str):
    if df_oc.empty:
        return
    df = df_oc.copy()
    df["snapshot_date"] = pd.to_datetime(df["snapshot_date"])
    df["frame_label"] = df["snapshot_date"].dt.strftime(date_fmt_py)
    frame_order = (df[["snapshot_date","frame_label"]]
                   .drop_duplicates().sort_values("snapshot_date")["frame_label"].tolist())
    sev_order = ["CRITICAL","HIGH","MEDIUM","LOW","UNKNOWN"]

    fig = px.bar(
        df.sort_values(["snapshot_date","Severity"]),
        x="Severity", y="count", color="Status",
        animation_frame="frame_label",
        category_orders={"Severity": sev_order, "frame_label": frame_order},
        labels={"count":"Vulnerability Count"},
        title="Open vs Closed by Severity (Animated, Stacked)"
              + (f" – {title_suffix}" if title_suffix else ""),
    )
    fig.update_layout(barmode="stack", template=theme, margin=dict(l=40,r=20,t=60,b=40))
    fig.update_yaxes(tickformat=",")
    fig.layout.updatemenus[0].buttons[0].args[1]["frame"]["duration"] = 450
    fig.layout.updatemenus[0].buttons[0].args[1]["transition"]["duration"] = 250
    fig.write_html(out_html, include_plotlyjs="inline", full_html=True)

# -------------------------
# Main
# -------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default=None, help="Path to config.yaml (CLI overrides config)")

    # Make --dir optional so config can provide it
    ap.add_argument("--dir", required=False, help="Directory containing the combined-prepped CSV files")
    ap.add_argument("--pattern", default="Daily Vulnerability Export - ALL - *combined - prepped.csv", help="Glob file pattern (non-recursive by default)")
    ap.add_argument("--recursive", action="store_true", help="Recurse into subfolders (uses rglob)")
    ap.add_argument("--outdir", default=".", help="Output directory for HTML files")

    ap.add_argument("--status-mode", choices=["open", "closed", "all"], default="open", help="Filter by status")
    ap.add_argument("--exploit", default=None, help="Optional exact match filter for 'Exploit status label'")

    ap.add_argument("--sep", default="\t", help="Delimiter override, e.g. ',' or '\\t' (PowerShell: use \"`t\" for tab)")
    ap.add_argument("--encoding", default="utf-8", help="File encoding")
    ap.add_argument("--skiprows", type=int, default=0, help="Rows to skip at top of each file")

    ap.add_argument("--until", default=None, help="Upper bound date (yyyymmdd). Defaults to latest file date.")
    ap.add_argument("--since", default=None, help="Lower bound date (yyyymmdd).")
    grp = ap.add_mutually_exclusive_group()
    grp.add_argument("--window_days", type=int, default=None, help="Lookback window in days")
    grp.add_argument("--window_weeks", type=int, default=None, help="Lookback window in weeks")

    ap.add_argument("--max-files", type=int, default=None, help="Process at most N files (after filters)")
    ap.add_argument("--chunksize", type=int, default=250000, help="Rows per chunk (<=0 reads whole file)")

    ap.add_argument("--allcols", action="store_true", help="Read all columns (disables usecols optimization)")
    ap.add_argument("--dry-run", action="store_true", help="List files that would be processed and exit")
    ap.add_argument("--verbose", action="store_true", help="Verbose progress logging")

    ap.add_argument("--topk", type=int, default=10, help="Number of products to show in bar race")
    ap.add_argument("--theme", default="plotly_dark", choices=["plotly", "plotly_dark", "simple_white", "ggplot2", "seaborn", "presentation"], help="Plot theme")

    args = ap.parse_args()

    # --- Config load (optional)
    cfg = {}
    if args.config:
        try:
            import yaml  # pip install pyyaml
            with open(args.config, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f) or {}
        except Exception as e:
            print(f"[WARN] Failed to load config: {e}")

    def choose(cli, *cfg_keys, default=None):
        cur = cli if cli not in (None, "", False) else None
        if cur is not None: return cur
        node = cfg
        for k in cfg_keys:
            node = node.get(k, {}) if isinstance(node, dict) else {}
        return node if node not in (None, "", False, {}) else default

    # Resolve settings (config → CLI override)
    dirpath   = Path(choose(args.dir, "paths", "dir"))
    outdir    = Path(choose(args.outdir, "paths", "outdir", default="."))
    pattern   = choose(args.pattern, "pattern", default=args.pattern)
    theme     = choose(args.theme, "theme", default=args.theme)
    sep       = normalize_sep(choose(args.sep, "sep", default=DEFAULT_SEP)) or DEFAULT_SEP
    encoding  = choose(args.encoding, "encoding", default=DEFAULT_ENCODING)
    top_k     = int(choose(args.topk, "topk", default=args.topk))
    show_vals = bool(choose(None, "show_values", default=True))
    status_mode = choose(args.status_mode, "status_mode", default=args.status_mode)

    cfg_window_weeks = choose(args.window_weeks, "window_weeks", default=None)
    cfg_window_days  = choose(args.window_days,  "window_days",  default=None)

    if not dirpath or not str(dirpath).strip():
        raise SystemExit("No --dir provided and no paths.dir set in config.")

    outdir.mkdir(parents=True, exist_ok=True)

    files_all = list_files(dirpath, pattern, args.recursive)

    # Build (file, yyyymmdd) list
    candidates = []
    for f in files_all:
        d = infer_yyyymmdd_str(f.name) or infer_yyyymmdd_str(str(f.parent))
        if d:
            candidates.append((f, d))
    if not candidates:
        raise SystemExit("No files with yyyymmdd in name found.")

    # Default "until" = latest file date if omitted
    all_dates = sorted(d for _, d in candidates)
    if args.until is None:
        args.until = all_dates[-1]

    # Compute since from window (CLI or config)
    if args.window_days or args.window_weeks or cfg_window_days or cfg_window_weeks:
        u = datetime.strptime(args.until, "%Y%m%d")
        days  = args.window_days if args.window_days else (cfg_window_days or 0)
        weeks = args.window_weeks if args.window_weeks else (cfg_window_weeks or 0)
        delta = timedelta(days=days) if days else timedelta(weeks=weeks)
        s = u - delta
        args.since = s.strftime("%Y%m%d")

    # Sanity clamp
    if args.since and args.until and args.since > args.until:
        args.since, args.until = args.until, args.since

    # Human-readable date strings & formats
    py_fmt = date_fmt_python(True)
    d3_fmt = date_fmt_d3(True)
    sincedate = datetime.strptime(args.since, "%Y%m%d") if args.since else datetime.strptime(all_dates[0], "%Y%m%d")
    untildate = datetime.strptime(args.until, "%Y%m%d")
    sincehr = sincedate.strftime(py_fmt)
    untilhr = untildate.strftime(py_fmt)
    title_suffix = f"{sincehr} – {untilhr}"

    # Filter files by date window
    annotated = [(f, d) for (f, d) in candidates if in_range(d, args.since, args.until)]
    annotated.sort(key=lambda x: x[1])  # asc
    if args.max_files and args.max_files > 0:
        annotated = annotated[-args.max_files:]

    if args.dry_run:
        print(f"[DRY RUN] Found {len(annotated)} files matching pattern/date filters.")
        for f, d in annotated[:20]:
            print(f"  {d}  {f.name}")
        if len(annotated) > 20:
            print(f"  ... and {len(annotated)-20} more")
        return

    if not annotated:
        raise SystemExit("No files to process after applying filters. Check --pattern/--since/--until.")

    # Open/Closed by severity (stacked)
    oc_rows = []
    for f, snap_raw in annotated:
        iso_date = yyyymmdd_to_iso(snap_raw)
        reader = pd.read_csv(
            f, sep=sep,
            engine=("c" if len(sep) == 1 else "python"),
            encoding=encoding, low_memory=False,
            usecols=["Severity","Status"], chunksize=250000
        )
        if not hasattr(reader, "__iter__"):
            reader = [reader]
        for chunk in reader:
            chunk["Severity"] = chunk["Severity"].astype(str).str.upper().fillna("UNKNOWN")
            chunk["Status"] = chunk["Status"].astype(str).str.upper().fillna("OPEN").replace({"REOPEN":"OPEN"})
            g = chunk.groupby(["Severity","Status"]).size().reset_index(name="count")
            g["snapshot_date"] = iso_date
            oc_rows.append(g)
    df_oc = pd.concat(oc_rows, ignore_index=True) if oc_rows else pd.DataFrame(columns=["snapshot_date","Severity","Status","count"])

    # Aggregate across all files (severity & product)
    sev_rows, prod_rows = [], []
    for f, snap_raw in annotated:
        iso_date = yyyymmdd_to_iso(snap_raw)
        if args.verbose:
            print(f"[READ] {iso_date}  {f}")

        sev_counter, prod_counter, *_ = load_and_aggregate_one(
            path=f, sep=sep, encoding=encoding, chunksize=args.chunksize,
            status_mode=status_mode, exploit_label=args.exploit,
            allcols=args.allcols, skiprows=args.skiprows, verbose=args.verbose,
        )
        for sev, cnt in sev_counter.items():
            sev_rows.append({"snapshot_date": iso_date, "severity": sev, "vuln_count": int(cnt)})
        for prod, cnt in prod_counter.items():
            prod_rows.append({"snapshot_date": iso_date, "product": prod, "vuln_count": int(cnt)})

    if not sev_rows or not prod_rows:
        raise SystemExit("No rows aggregated; check filters or input columns.")

    df_sev = pd.DataFrame(sev_rows)
    df_prod = pd.DataFrame(prod_rows)

    # Stable color map: only for products that actually appear in Top-K (any frame)
    top_per_date = (df_prod.sort_values(["snapshot_date","vuln_count"], ascending=[True, False])
                            .groupby("snapshot_date", as_index=False)
                            .head(top_k))
    window_products = top_per_date["product"].astype(str).dropna().unique().tolist()

    color_json = outdir / "product_colors.json"
    base_map = load_product_colormap(color_json)
    prod_cmap = build_product_colormap(window_products, base_map)
    save_product_colormap(color_json, prod_cmap)

    # Build HTMLs
    make_severity_animation(
        df_sev,
        outdir / f"animated_severity_trend-{args.until}.html",
        theme=theme,
        date_fmt_py=py_fmt,
        date_fmt_d3_str=d3_fmt,
        title_suffix=title_suffix,
    )

    make_open_closed_stacked(
        df_oc,
        outdir / f"open_closed_by_severity-{args.until}.html",
        theme=theme,
        date_fmt_py=py_fmt,
        title_suffix=title_suffix,
    )

    make_bar_race(
        df_prod,
        outdir / f"animated_bar_race_products-{args.until}.html",
        top_k=top_k,
        theme=theme,
        show_values=show_vals,
        date_fmt_py=py_fmt,
        title_suffix=title_suffix,
        color_map=prod_cmap,
    )

    print("[DONE] Wrote:")
    print(" ", outdir / f"animated_severity_trend-{args.until}.html")
    print(" ", outdir / f"animated_bar_race_products-{args.until}.html")
    print(" ", outdir / f"open_closed_by_severity-{args.until}.html")

if __name__ == "__main__":
    main()
